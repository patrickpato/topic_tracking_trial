{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Text', 'Unnamed: 2', 'Unnamed: 3'], dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('AfterRona.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-29 05:29:40</td>\n",
       "      <td>b'Today is tomorrow\\xe2\\x80\\x99s history, wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-29 01:29:43</td>\n",
       "      <td>b\"Can't wait to see other tech that'll come up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-28 23:07:14</td>\n",
       "      <td>b\"RT @Silvia_Wangeci: A woman shows her middle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-28 21:05:02</td>\n",
       "      <td>b'Pigs freak me out but I\\xe2\\x80\\x99m still d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-28 20:29:12</td>\n",
       "      <td>b'RT @Bernardkiprop3: #KOT Help RT this until ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                               Text\n",
       "0  2020-05-29 05:29:40  b'Today is tomorrow\\xe2\\x80\\x99s history, wond...\n",
       "1  2020-05-29 01:29:43  b\"Can't wait to see other tech that'll come up...\n",
       "2  2020-05-28 23:07:14  b\"RT @Silvia_Wangeci: A woman shows her middle...\n",
       "3  2020-05-28 21:05:02  b'Pigs freak me out but I\\xe2\\x80\\x99m still d...\n",
       "4  2020-05-28 20:29:12  b'RT @Bernardkiprop3: #KOT Help RT this until ..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['Unnamed: 2', 'Unnamed: 3'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2641 entries, 0 to 2640\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    2641 non-null   object\n",
      " 1   Text    2641 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 41.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stopwords = ['brt', 'rt']\n",
    "urls = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' 'rt')\n",
    "usernames = re.compile(\"@[A-Za-z0-9]+\")\n",
    "def text_cleaning(text):\n",
    "    text = usernames.sub(' ', text)#removing usernames\n",
    "    text = BeautifulSoup(text, \"lxml\").text #removing any html decoding\n",
    "    text = text.lower() #removing capitalization\n",
    "    text = space_replace.sub(' ', text)#replacing symbols with a space\n",
    "    text = bad_symbols.sub('', text) #deleting symbols from the text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) #removing stopwords\n",
    "    text = urls.sub('', text)#removing urls\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "data['cleaned_text'] = data['Text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-29 05:29:40</td>\n",
       "      <td>b'Today is tomorrow\\xe2\\x80\\x99s history, wond...</td>\n",
       "      <td>btoday is tomorrowxe2x80x99s history wondering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-29 01:29:43</td>\n",
       "      <td>b\"Can't wait to see other tech that'll come up...</td>\n",
       "      <td>bcant wait to see other tech thatll come up af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-28 23:07:14</td>\n",
       "      <td>b\"RT @Silvia_Wangeci: A woman shows her middle...</td>\n",
       "      <td>wangeci a woman shows her middle finger to tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-28 21:05:02</td>\n",
       "      <td>b'Pigs freak me out but I\\xe2\\x80\\x99m still d...</td>\n",
       "      <td>bpigs freak me out but ixe2x80x99m still down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-28 20:29:12</td>\n",
       "      <td>b'RT @Bernardkiprop3: #KOT Help RT this until ...</td>\n",
       "      <td>kot help this until recognises this beautiful ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                               Text  \\\n",
       "0  2020-05-29 05:29:40  b'Today is tomorrow\\xe2\\x80\\x99s history, wond...   \n",
       "1  2020-05-29 01:29:43  b\"Can't wait to see other tech that'll come up...   \n",
       "2  2020-05-28 23:07:14  b\"RT @Silvia_Wangeci: A woman shows her middle...   \n",
       "3  2020-05-28 21:05:02  b'Pigs freak me out but I\\xe2\\x80\\x99m still d...   \n",
       "4  2020-05-28 20:29:12  b'RT @Bernardkiprop3: #KOT Help RT this until ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  btoday is tomorrowxe2x80x99s history wondering...  \n",
       "1  bcant wait to see other tech thatll come up af...  \n",
       "2  wangeci a woman shows her middle finger to tru...  \n",
       "3  bpigs freak me out but ixe2x80x99m still down ...  \n",
       "4  kot help this until recognises this beautiful ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A VOCUBLARY FROM THE DATA USING COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2641x1307 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "term_matrix =  count_vect.fit_transform(data['cleaned_text'].values.astype('U')) #including words that occur less than 80% of the time in the document\n",
    "'''stop words have also been removed since they barely contribute significantly to the vocabulary'''\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#every word in the document is represented by a 1307 dimensional vector. i.e we have a vocabulary of 1307 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we now use LDA to create topics based on the probability of each word in the document\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42) #we set n = 5 as our initial guess of topics in the data\n",
    "lda.fit(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despite\n",
      "spread\n",
      "girlnafterrona\n",
      "delegationxf0x9fx92xa5xf0x9fx92xa5xf0x9fx92xa5nnpogbabrunomarcus\n",
      "bme\n",
      "start\n",
      "officers\n",
      "washing\n",
      "ntiktok\n",
      "bafter\n",
      "club\n",
      "son\n",
      "globalsundaysexe2x80xa6\n",
      "better\n",
      "nini\n",
      "breathtaking\n",
      "niggas\n",
      "forge\n",
      "startxe2x80xa6\n",
      "weaknessxe2x80xa6\n",
      "reasons\n",
      "retweet\n",
      "virus\n",
      "xe2x99xa5xefxb8x8f\n",
      "mother\n",
      "stayed\n",
      "ounce\n",
      "4songs\n",
      "coward\n",
      "cancel\n",
      "account\n",
      "issue\n",
      "isolationxf0x9fx98x82xf0x9fx98x82xf0x9fx98x82nafterronaneidmubarak\n",
      "joy\n",
      "food\n",
      "photos\n",
      "acting\n",
      "churchillshow\n",
      "amp\n",
      "deapstate\n",
      "cocacola\n",
      "really\n",
      "rice\n",
      "strength\n",
      "neidmubarak\n",
      "qmbbvkm4re\n",
      "imagine\n",
      "gonna\n",
      "afterronannthis\n",
      "great\n",
      "metres\n"
     ]
    }
   ],
   "source": [
    "#top 50 words in the vocubulary\n",
    "for i in range(51):\n",
    "    random_word = random.randint(0, len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20002213,  0.20069079,  2.19997897, ...,  2.19998035,\n",
       "        5.19998942, 10.19999216])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the first topic\n",
    "first_topic = lda.components_[0]\n",
    "first_topic# the output is a vector. from the vector we can then obtain the words from the count_vectorizer feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp\n",
      "people\n",
      "retweet\n",
      "like\n",
      "kenya\n",
      "miguna\n",
      "bafterrona\n",
      "afterrona\n",
      "https\n",
      "tco\n"
     ]
    }
   ],
   "source": [
    "#obtaining the top words in the first topic\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['monitor', 'donxe2x80x99t', 'just', 'owino', 'timelinenni', 'lotxf0x9fx99x8fxf0x9fx8fxbfnmy', 'client', 'efficient', 'means', 'offer', 'amp', 'people', 'retweet', 'like', 'kenya', 'miguna', 'bafterrona', 'afterrona', 'https', 'tco']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['trend', 'little', 'let', 'girl', 'cocacola', 'make', 'herxe2x80xa6', 'quenching', 'ifikie', 'recognises', 'help', 'photo', 'kot', 'kenyan', 'beautiful', 'child', 'young', 'afterrona', 'tco', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['nkot', 'young', 'face', 'kid', 'hey', 'retweet', 'deserves', 'baringo', 'possesses', 'research', 'researchers', 'intelligence', 'stones', 'thanxe2x80xa6', 'bucket', 'institute', 'lulli', 'medical', 'feeling', 'kenya']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['nthe', 'park', 'number', 'places', 'bexe2x80xa6', 'months', 'future', 'place', 'know', 'trees', 'motorcadenrutowantedtokillxe2x80xa6', 'wangeci', 'trumps', 'middle', 'motorcade', 'uhurus', 'finger', 'woman', 'shows', 'afterrona']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['xf0x9fx8exa5xe2x80xa6', 'nvideo', 'decides', 'nmigunanafterrona', 'hillary', 'courtesy', 'nrentakikuyu', 'going', 'finish', 'woman', 'afxe2x80xa6', 'refreshment', 'epitome', 'billboards', 'kids', 'cocacolaninstead', 'coca', 'cola', 'rich', 'dont']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the top 20 words in each of the topics\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC TRACKING USING NON-NEGATIVE MATRIX FACTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "term_matrix_tfidf = tfidf_vect.fit_transform(data['cleaned_text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2641x1307 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(term_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nduale\n",
      "hands\n",
      "nexe2x80xa6\n",
      "possesses\n",
      "streets\n",
      "mburru\n",
      "xe2x99xa5xefxb8x8f\n",
      "piece\n",
      "end\n",
      "oil\n"
     ]
    }
   ],
   "source": [
    "#obtaining random words from our new vocab\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retweet\n",
      "kid\n",
      "thirst\n",
      "nkot\n",
      "tasting\n",
      "cocacolaxe2x80xa6\n",
      "face\n",
      "deserves\n",
      "feeling\n",
      "thxe2x80xa6\n",
      "young\n",
      "quenching\n",
      "help\n",
      "herxe2x80xa6\n",
      "kenyan\n",
      "child\n",
      "beautiful\n",
      "kot\n",
      "photo\n",
      "recognises\n"
     ]
    }
   ],
   "source": [
    "#top words in the first topic\n",
    "first_topic = nmf.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-20:]\n",
    "for i in top_topic_words:\n",
    "    print(tfidf_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['retweet', 'kid', 'thirst', 'nkot', 'tasting', 'cocacolaxe2x80xa6', 'face', 'deserves', 'feeling', 'thxe2x80xa6', 'young', 'quenching', 'help', 'herxe2x80xa6', 'kenyan', 'child', 'beautiful', 'kot', 'photo', 'recognises']\n",
      "\n",
      "\n",
      "Top 20 words for topic #1:\n",
      "['entire', 'amazing', 'sees', 'lets', 'invest', 'lessons', 'qmtft1tcj5', 'covid19', 'know', 'https', 'tco', 'afterrona', 'ifikie', 'make', 'cocacola', 'trend', 'girl', 'let', 'little', 'aunouwdicb']\n",
      "\n",
      "\n",
      "Top 20 words for topic #2:\n",
      "['lessons', 'qmtft1tcj5', 'going', 'finish', 'hillary', 'xf0x9fx8exa5xe2x80xa6', 'nmigunanafterrona', 'decides', 'courtesy', 'nvideo', 'afterrona', 'woman', 'shows', 'motorcadenrutowantedtokillxe2x80xa6', 'wangeci', 'motorcade', 'finger', 'middle', 'trumps', 'uhurus']\n",
      "\n",
      "\n",
      "Top 20 words for topic #3:\n",
      "['lessons', 'qmtft1tcj5', 'covid19', 'nvideo', 'decides', 'xf0x9fx8exa5xe2x80xa6', 'nmigunanafterrona', 'courtesy', 'hillary', 'finish', 'going', 'https', 'tco', 'ifikie', 'nrentakikuyu', 'ljl82mwvxe2x80xa6', 'daggy', 'nmigunancanadanisrael', 'shynafterrona', 'nnpictures']\n",
      "\n",
      "\n",
      "Top 20 words for topic #4:\n",
      "['tastethefeeling', 'want', 'better', 'work', 'bafterrona', 'cocacolagirl', 'bthis', 'cocaxe2x80xa6', 'https', 'tco', 'dont', 'rich', 'cola', 'coca', 'afxe2x80xa6', 'refreshment', 'epitome', 'cocacolaninstead', 'kids', 'billboards']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#obtaining top 20 words for each of the topics\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook was to attempt to find a means of identifying the main topics under a given twitter hashtag. \n",
    "I decided to use the #AfterROna hashtag for this exercise, as well as text clustering using SKlearn's Latent DirichletAllocation and Non-negative matrix factorization. \n",
    "The results indicate that local Kenyan politics and the Baringo girl were the most discussed topics under the hashtag. \n",
    "The main challenge experienced in this exercise was to properly prepare the dataset. Various unreadable words were captured as the top words and this indicates that a significant portion of the data was not clean. \n",
    "This will be improved on in future exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
